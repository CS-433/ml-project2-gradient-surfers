{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from utils import *\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "\n",
    "class PolymerDataset(Dataset):\n",
    "    def __init__(self, data_paths, timesteps=100) -> None:\n",
    "        self.raw_data = [np.load(data_path, allow_pickle=True) for data_path in data_paths]\n",
    "        self.prepare(timesteps=timesteps)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.labels[idx]\n",
    "    \n",
    "    def _process_event(self, event, timesteps=100, diff_threshold=0):\n",
    "        compressed_event = []\n",
    "        step_size = int(np.ceil(len(event) / timesteps))\n",
    "        for i in range(timesteps):\n",
    "            sub_event = event[i*step_size:(i+1)*step_size]\n",
    "            features = build_features(sub_event, diff_threshold=diff_threshold)\n",
    "            compressed_event.append(np.array(list(features.values())))\n",
    "        return np.array(compressed_event)\n",
    "\n",
    "    def prepare(self, timesteps=100, diff_threshold=0):\n",
    "        data = []\n",
    "        labels = []\n",
    "\n",
    "        for data_index, raw_data in enumerate(self.raw_data):\n",
    "            for event in raw_data:\n",
    "                processed_event = self._process_event(event, timesteps=timesteps, diff_threshold=diff_threshold)\n",
    "                data.append(processed_event)\n",
    "                labels.append(data_index)\n",
    "\n",
    "        self.data = torch.tensor(np.array(data), dtype=torch.float)\n",
    "        self.labels = torch.tensor(np.array(labels), dtype=torch.long)\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = PolymerDataset(['data/AA66266AA.npy', 'data/AA66466AA.npy', 'data/AA66566AA.npy'], timesteps=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([140153, 64, 8]), torch.Size([140153]))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.data.shape, dataset.labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import random_split\n",
    "train_size = int(0.8 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "train_data, test_data = random_split(dataset, [train_size, test_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(22039), tensor(75040), tensor(43074))"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(dataset.labels == 0).sum(), (dataset.labels == 1).sum(), (dataset.labels == 2).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "class PolymerLSTM(torch.nn.Module):\n",
    "    def __init__(self, num_features, num_classes, hidden_size=32) -> None:\n",
    "        super().__init__()\n",
    "        self.lstm = torch.nn.LSTM(input_size=num_features, hidden_size=hidden_size, batch_first=True)\n",
    "        self.linear = torch.nn.Linear(hidden_size, num_classes)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        lstm_out, _ = self.lstm(X)\n",
    "        outputs = lstm_out[:, -1, :]\n",
    "        outputs = self.linear(outputs)\n",
    "        probs = torch.nn.functional.log_softmax(outputs, dim=1)\n",
    "        return probs\n",
    "    \n",
    "    def predict(self, X):\n",
    "        probs = self.forward(X)\n",
    "        preds = torch.argmax(probs, dim=1, keepdim=False)\n",
    "        return preds\n",
    "\n",
    "\n",
    "def train(dataset, num_epochs=100, batch_size=64, num_features=2, num_classes=2):\n",
    "    data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    model = PolymerLSTM(num_features, num_classes)\n",
    "    loss_function = torch.nn.NLLLoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        num_correct = 0\n",
    "        for X, y in iter(data_loader):\n",
    "            model.zero_grad()\n",
    "            probs = model(X)\n",
    "            loss = loss_function(probs, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            preds = torch.argmax(probs, dim=1, keepdim=False)\n",
    "            num_correct += (preds == y).sum()\n",
    "        print(f'epoch={epoch}/{num_epochs}, loss={loss}, accuracy={num_correct*100/len(dataset)}')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=0/100, loss=0.8802793622016907, accuracy=55.40215301513672\n",
      "epoch=1/100, loss=0.9036217331886292, accuracy=57.134193420410156\n",
      "epoch=2/100, loss=0.8428705334663391, accuracy=57.650596618652344\n",
      "epoch=3/100, loss=0.8184763193130493, accuracy=58.44169616699219\n",
      "epoch=4/100, loss=1.013095498085022, accuracy=59.25331497192383\n",
      "epoch=5/100, loss=0.7359552979469299, accuracy=60.13449478149414\n",
      "epoch=6/100, loss=0.8546602129936218, accuracy=61.260948181152344\n",
      "epoch=7/100, loss=0.811974287033081, accuracy=61.978023529052734\n",
      "epoch=8/100, loss=0.7188849449157715, accuracy=62.30088806152344\n",
      "epoch=9/100, loss=0.7336299419403076, accuracy=62.979610443115234\n",
      "epoch=10/100, loss=0.7796737551689148, accuracy=63.37025833129883\n",
      "epoch=11/100, loss=0.7835960388183594, accuracy=63.62444305419922\n",
      "epoch=12/100, loss=0.7308964729309082, accuracy=63.96603775024414\n",
      "epoch=13/100, loss=0.7248246669769287, accuracy=64.26214599609375\n",
      "epoch=14/100, loss=0.7424956560134888, accuracy=64.52168273925781\n",
      "epoch=15/100, loss=0.924858808517456, accuracy=64.58768463134766\n",
      "epoch=16/100, loss=0.8696274757385254, accuracy=64.72592163085938\n",
      "epoch=17/100, loss=0.6714813113212585, accuracy=64.75714111328125\n",
      "epoch=18/100, loss=0.7716175317764282, accuracy=64.70005798339844\n",
      "epoch=19/100, loss=0.793877899646759, accuracy=65.04700469970703\n",
      "epoch=20/100, loss=0.6790367364883423, accuracy=65.17097473144531\n",
      "epoch=21/100, loss=0.6682589650154114, accuracy=65.19416046142578\n",
      "epoch=22/100, loss=0.7344265580177307, accuracy=65.39483642578125\n",
      "epoch=23/100, loss=0.7098466157913208, accuracy=65.54913330078125\n",
      "epoch=24/100, loss=0.6703363656997681, accuracy=65.70164489746094\n",
      "epoch=25/100, loss=0.86396324634552, accuracy=65.73731994628906\n",
      "epoch=26/100, loss=0.8265143632888794, accuracy=65.93175506591797\n",
      "epoch=27/100, loss=0.8149727582931519, accuracy=65.99418640136719\n",
      "epoch=28/100, loss=0.7232274413108826, accuracy=65.9290771484375\n",
      "epoch=29/100, loss=0.6560050845146179, accuracy=66.18058776855469\n",
      "epoch=30/100, loss=0.7478869557380676, accuracy=66.20199584960938\n",
      "epoch=31/100, loss=0.738459050655365, accuracy=66.24212646484375\n",
      "epoch=32/100, loss=0.6526542901992798, accuracy=66.23588562011719\n",
      "epoch=33/100, loss=0.683988630771637, accuracy=66.33488464355469\n",
      "epoch=34/100, loss=0.6723201870918274, accuracy=66.52485656738281\n",
      "epoch=35/100, loss=0.6745577454566956, accuracy=66.45796203613281\n",
      "epoch=36/100, loss=0.6307699084281921, accuracy=66.58015441894531\n",
      "epoch=37/100, loss=0.7603840231895447, accuracy=66.69788360595703\n",
      "epoch=38/100, loss=0.6180262565612793, accuracy=66.74158477783203\n",
      "epoch=39/100, loss=0.6934009194374084, accuracy=66.9039077758789\n",
      "epoch=40/100, loss=0.8753207921981812, accuracy=66.89141845703125\n",
      "epoch=41/100, loss=0.700341522693634, accuracy=67.0564193725586\n",
      "epoch=42/100, loss=0.6595243215560913, accuracy=66.84326171875\n",
      "epoch=43/100, loss=0.7301677465438843, accuracy=66.82363891601562\n",
      "epoch=44/100, loss=0.6934343576431274, accuracy=66.92977142333984\n",
      "epoch=45/100, loss=0.6371879577636719, accuracy=66.91461181640625\n",
      "epoch=46/100, loss=0.7037135362625122, accuracy=67.09298706054688\n",
      "epoch=47/100, loss=0.6337811946868896, accuracy=67.06177520751953\n",
      "epoch=48/100, loss=0.568391740322113, accuracy=67.09120178222656\n",
      "epoch=49/100, loss=0.7219315767288208, accuracy=67.05552673339844\n",
      "epoch=50/100, loss=0.7033482193946838, accuracy=67.25531005859375\n",
      "epoch=51/100, loss=0.6880239248275757, accuracy=67.33647155761719\n",
      "epoch=52/100, loss=0.7878329753875732, accuracy=67.33112335205078\n",
      "epoch=53/100, loss=0.7671838998794556, accuracy=67.03055572509766\n",
      "epoch=54/100, loss=0.9112358093261719, accuracy=67.13401794433594\n",
      "epoch=55/100, loss=0.6707758903503418, accuracy=67.32666015625\n",
      "epoch=56/100, loss=0.6295107007026672, accuracy=67.35966491699219\n",
      "epoch=57/100, loss=0.640252411365509, accuracy=67.34271240234375\n",
      "epoch=58/100, loss=0.6653816103935242, accuracy=67.54338836669922\n",
      "epoch=59/100, loss=0.790543794631958, accuracy=67.45777130126953\n",
      "epoch=60/100, loss=0.59055095911026, accuracy=67.44528198242188\n",
      "epoch=61/100, loss=0.5588122010231018, accuracy=67.52555084228516\n",
      "epoch=62/100, loss=0.6114771962165833, accuracy=67.60939025878906\n",
      "epoch=63/100, loss=0.6917304396629333, accuracy=67.6521987915039\n",
      "epoch=64/100, loss=0.6758700609207153, accuracy=67.59868621826172\n",
      "epoch=65/100, loss=0.7658994197845459, accuracy=67.66468811035156\n",
      "epoch=66/100, loss=0.6666117906570435, accuracy=67.90995788574219\n",
      "epoch=67/100, loss=0.7079704403877258, accuracy=67.87873840332031\n",
      "epoch=68/100, loss=0.6574962139129639, accuracy=68.05622100830078\n",
      "epoch=69/100, loss=0.6681540608406067, accuracy=67.86803436279297\n",
      "epoch=70/100, loss=0.7148334383964539, accuracy=68.04373931884766\n",
      "epoch=71/100, loss=0.7381924390792847, accuracy=67.98665618896484\n",
      "epoch=72/100, loss=0.7914812564849854, accuracy=68.05711364746094\n",
      "epoch=73/100, loss=0.6192110776901245, accuracy=68.22479248046875\n",
      "epoch=74/100, loss=0.5835579633712769, accuracy=68.19803619384766\n",
      "epoch=75/100, loss=0.743900716304779, accuracy=68.29167938232422\n",
      "epoch=76/100, loss=0.6021615862846375, accuracy=68.11687469482422\n",
      "epoch=77/100, loss=0.7157951593399048, accuracy=68.35322570800781\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/rs/c9bqjyq95q59ngc5v1t0gz_00000gn/T/ipykernel_20746/2229862056.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/folders/rs/c9bqjyq95q59ngc5v1t0gz_00000gn/T/ipykernel_20746/2562088270.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(dataset, num_epochs, batch_size, num_features, num_classes)\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m             \u001b[0mprobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/ml/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/rs/c9bqjyq95q59ngc5v1t0gz_00000gn/T/ipykernel_20746/2562088270.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mlstm_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlstm_out\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/ml/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/ml/lib/python3.9/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    689\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_forward_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 691\u001b[0;31m             result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n\u001b[0m\u001b[1;32m    692\u001b[0m                               self.dropout, self.training, self.bidirectional, self.batch_first)\n\u001b[1;32m    693\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = train(train_data, num_features=dataset.data.shape[2], num_classes=3, batch_size=128)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "34e9dc8c9cd4c2e3341692c7f5472da17e27c062a0f2ac63648b60e63867ef4a"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('ml': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
