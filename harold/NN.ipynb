{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## classic pydata stack\n",
    "import os \n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "%matplotlib inline \n",
    "\n",
    "plt.style.use('ggplot')\n",
    "plt.rcParams['figure.figsize'] = (15,7)\n",
    "\n",
    "\n",
    "\n",
    "## torch\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "## SEEDING\n",
    "\n",
    "torch.manual_seed(1)\n",
    "\n",
    "\n",
    "REBUILD_DATA = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Report notes\n",
    "\n",
    "- need to filter out signals that are too long because the length distribution between the two datasets is very important\n",
    "- need to balance datasets because one dataset is much bigger than the other\n",
    "- process the data such that an RNN or any NN can process it\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Input():\n",
    "    def __init__(self, raw_series,num_blocks,label):\n",
    "        \"\"\" Initilaizes an input object from a raw time series i.e. an input suitable to feed to a recurrent neural network\n",
    "\n",
    "        Args:\n",
    "            raw_series (numpy array of shape (num_timesteps,2)): raw time series from npy data i.e. arr[0] where arr = np.load(\"data.npy\")\n",
    "            num_blocks ([type]): number of \"feature blocks\" into which the time series will be sliced i.e the number of of times we need to feed \n",
    "            to the LSTM to train on the entire time series\n",
    "            label ([type]): Whether it was a \"00\" backbone (label:0) or a \"66\" backbone (label:1)\n",
    "        \"\"\"\n",
    "\n",
    "        self.label = label\n",
    "        self.input = self.process(raw_series,num_blocks)\n",
    "\n",
    "\n",
    "    def process(self,raw_series,num_blocks):\n",
    "        \"\"\" Function that does the entire processing of going from raw time series to a suitable input to feed to a recurrent neural network\n",
    "\n",
    "        Args:\n",
    "            raw_series (numpy array of shape (num_timesteps,2)): raw time series from npy data i.e. arr[0] where arr = np.load(\"data.npy\")\n",
    "            num_blocks ([type]): number of \"feature blocks\" into which the time series will be sliced i.e the number of of times we need to feed \n",
    "            to the LSTM to train on the entire time series\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: array of features from a single raw time series instance\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "        # stores the processed time series\n",
    "        res = np.array([])\n",
    "\n",
    "        ## returns a list of transformed time series (current list: normal. lowpass filtered, highpass filtered)\n",
    "        instances = self.transform(raw_series)\n",
    "\n",
    "\n",
    "        for instance in instances:\n",
    "            ## chunks an instance of a time series into blocks and extract feature from each block\n",
    "            extracted = self.extract_features(instance,num_blocks)\n",
    "            res = np.concatenate((res,extracted),axis=None)\n",
    "\n",
    "        return res\n",
    "\n",
    "\n",
    "    def transform(self,raw_series):\n",
    "        \"\"\" Given a raw time series, outputs several transformations applied to it\n",
    "            Transformations may be filtering, projecting, ...\n",
    "\n",
    "        Args:\n",
    "            raw_series numpy.ndarray : 1 dimensional array representing the current values\n",
    "\n",
    "        Returns:\n",
    "            List(numpy.ndarray): list of all transformations\n",
    "        \"\"\"\n",
    "\n",
    "        res = [raw_series]\n",
    "\n",
    "        return res\n",
    "\n",
    "    def extract_features(self,instance, num_blocks):\n",
    "\n",
    "        \"\"\" From a time series, divides it into num_blocks blocks and from each block, extract numerical features usable for a neural network\n",
    "\n",
    "        Args:\n",
    "            instance (numpy.ndarray): 1D array containing numerical values\n",
    "            num_blocks (int): number of \"feature blocks\" into which the time series will be sliced i.e the number of of times we need to feed \n",
    "            to the LSTM to train on the entire time series\n",
    "\n",
    "\n",
    "        Returns:\n",
    "            numpy.ndarray: 1D array of length num_blocks*num_features_per_block containing all the features from a time series\n",
    "        \"\"\"\n",
    "\n",
    "        res = np.array([])\n",
    "        length = len(instance)\n",
    "        # divide the length by num_blocks to get block_size\n",
    "        block_size, remainder  = divmod(length,num_blocks)\n",
    "\n",
    "\n",
    "        # iterating over each block and extracting features\n",
    "        for i in range(num_blocks):\n",
    "\n",
    "            curr = instance[block_size*i: block_size*(i+1)]\n",
    "            # get features from block (mean, std, length, ...)\n",
    "            features = self.features(curr)\n",
    "            res = np.concatenate((res, features),axis=None)\n",
    "\n",
    "\n",
    "        ## get the remainder of the time series\n",
    "        ##curr = instance[block_size*num_blocks:]\n",
    "        ##features = self.features(curr)\n",
    "        ##res = np.concatenate((res,features),axis=None)\n",
    "\n",
    "        return res\n",
    "\n",
    "    def features(self,instance):\n",
    "        \"\"\"\n",
    "        From a block of a time series, extracts numerical features usable for a neural network\n",
    "        Args:\n",
    "            instance (numpy.ndarray): 1D array containing numerical values \n",
    "        \"\"\"\n",
    "        res = np.array([])\n",
    "\n",
    "        # list of functions applied to the array for feature extraction\n",
    "        functions = [np.mean,np.median,np.std,np.min,np.max,len]\n",
    "\n",
    "        for func in functions:\n",
    "            res = np.concatenate((res,func(instance)),axis=None)\n",
    "\n",
    "        \n",
    "\n",
    "        return res        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class PolymerDataset(Dataset):\n",
    "\n",
    "    ## These functions are necessary to define an iterator usable by Pytorch\n",
    "\n",
    "    def __init__(self, data_paths,num_blocks, lstm=False, seed=10):\n",
    "        super().__init__()\n",
    "        self.process(data_paths,num_blocks,lstm,seed)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.labels[idx]\n",
    "\n",
    "\n",
    "    def process(self,data_paths, num_blocks,lstm,seed):\n",
    "        \"\"\" Processes the two datasets in the aim of not having bias catchable by the neural network:\n",
    "        - filtering signals that are too long and too short\n",
    "        - balancing the two datasets, resulting in the two classes each representing 50% of the data\n",
    "        - Process each of the raw time series current into suitable inputs\n",
    "        - Output a dataset where each row represents a suitable input for a NN derived from the raw time series\n",
    "\n",
    "        Args:\n",
    "            data_paths (list[string]): Should be a list of length 2 containing the paths of the data to be loaded \n",
    "            num_blocks (int): number of \"feature blocks\" into which a raw time series will be sliced i.e the number of of times we need to feed the\n",
    "            to the LSTM to train on the entire time series\n",
    "            seed (int): for setting the seed\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        raw_data = [np.load(data_path, allow_pickle=True) for data_path in data_paths]\n",
    "        labels = [0,1]\n",
    "\n",
    "    ## balance the dataset by removing signals that are too short or too long\n",
    "    ## first we build the dataframe to know the lengths of the time series\n",
    "\n",
    "        len_series = []\n",
    "\n",
    "        for data in raw_data:\n",
    "            lengths= []\n",
    "            for row in data:\n",
    "                ## length of time series\n",
    "                lengths.append(row.shape[0])\n",
    "\n",
    "            len_series.append(pd.Series(lengths))\n",
    "\n",
    "        ## enforces that the first dataset is the smaller one in total size\n",
    "        ## such that we can apply our balancing operations generally\n",
    "        if len(len_series[0]) > len(len_series[1]):\n",
    "            len_series.reverse()\n",
    "            raw_data.reverse()\n",
    "            labels.reverse()\n",
    "\n",
    "        ## filter the dataset and remove signals that are:\n",
    "        ## too short i.e. < len_series[0].quantile(0.1)\n",
    "        ## too long i.e. > len_series[0].quantile(0.9)\n",
    "        for i in range(2):\n",
    "            mask = (len_series[i] > len_series[0].quantile(0.1)) & (len_series[i] < len_series[0].quantile(0.9))\n",
    "            raw_data[i] = raw_data[i][mask]\n",
    "\n",
    "        ## most likely, one dataset is still bigger than the other one\n",
    "        ## therefore, we randomly sample data from the bigger dataset to create a new dataset of the same size as the small one \n",
    "        np.random.seed(seed=seed)\n",
    "\n",
    "        # making sure the smallest dataset is the first one\n",
    "        if len(raw_data[0]) > len(raw_data[1]):\n",
    "            raw_data.reverse()\n",
    "            labels.reverse()\n",
    "\n",
    "        # randomly sampling and making a balanced dataset\n",
    "        raw_data[1]  = np.random.permutation(raw_data[1])[:len(raw_data[0])]\n",
    "        data=[]\n",
    "        data_labels=[]\n",
    "        \n",
    "        ## using our Input class to build the entire dataset and extracting features from each row\n",
    "        for index, raw_data in enumerate(raw_data):\n",
    "            for raw_series in raw_data:\n",
    "                processed_series = Input(raw_series=raw_series,num_blocks=num_blocks,label=labels[index])\n",
    "                data.append(processed_series.input)\n",
    "                data_labels.append(labels[index])\n",
    "        data = np.array(data)\n",
    "\n",
    "        #normalizing features\n",
    "        data = (data - data.mean(axis=0)) / data.std(axis=0)\n",
    "\n",
    "        data = torch.Tensor(data).float()\n",
    "\n",
    "        ## if lstm is true, set up the data such that it can easily be fed into a lstm\n",
    "        if lstm:\n",
    "            data = data.view((data.shape[0],num_blocks,-1))\n",
    "\n",
    "        self.data = data\n",
    "        self.labels = torch.Tensor(np.array(data_labels)).long()\n",
    "\n",
    "        return self\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "class LSTM(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim, num_layers, hidden_dim):\n",
    "        \"\"\" creates a lstm neural network\n",
    "\n",
    "        Args:\n",
    "            input_dim (int)): Defines the dimension of the input x, should be equal to the number of features extracted per block\n",
    "            num_layers(int): Defines the number of LSTM layers, should be equal to num_blocks\n",
    "            hidden_dim (int): defines the number of features in the hidden states\n",
    "        \"\"\"\n",
    "        super(LSTM, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size= input_dim, num_layers=num_layers, hidden_size=hidden_dim,batch_first=True)\n",
    "        self.fc1 =nn.Linear(hidden_dim, 2)\n",
    "\n",
    "    \n",
    "    def forward(self, input):\n",
    "        \"\"\" Forward pass of our network\n",
    "\n",
    "        Args:\n",
    "            input ([type]): should be our current time series preprocessed with shape(num_blocks, num_features) \n",
    "            where num_blocks is the number of blocks in which we have divided our time series and  num_features is the number of feature per block\n",
    "        \"\"\"\n",
    "        num_blocks=input.shape[0]\n",
    "        \n",
    "        ## the LSTM output are the hidden states values for all hidden states while processing the sequence\n",
    "        lstm_out, _ = self.lstm(input)\n",
    "\n",
    "        ## we only want last hidden states values\n",
    "        lstm_out = lstm_out[:,-1,:]\n",
    "        ## passing through MLP and softmax\n",
    "        last = self.fc1(lstm_out.view(num_blocks,-1))\n",
    "        scores = F.log_softmax(last,dim=1)\n",
    "\n",
    "        return scores\n",
    "\n",
    "    def predict(self, test_data):\n",
    "        probs = self.forward(X)\n",
    "        preds = torch.argmax(probs, dim=1, keepdim=False)\n",
    "        return preds\n",
    "\n",
    "    def train(dataset, num_features, num_blocks, hidden_dim, num_epochs, batch_size):\n",
    "\n",
    "        data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "        model = LSTM(input_dim = num_features, num_layers= num_blocks ,hidden_dim = hidden_dim)\n",
    "        loss_function = torch.nn.NLLLoss()\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            num_correct = 0\n",
    "            for X, y in iter(data_loader):\n",
    "                model.zero_grad()\n",
    "                probs = model(X)\n",
    "                loss = loss_function(probs, y)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                preds = torch.argmax(probs, dim=1, keepdim=False)\n",
    "                num_correct += (preds == y).sum()\n",
    "            print(f'epoch={epoch}/{num_epochs - 1}, loss={loss}, accuracy={num_correct*100/len(dataset)}')\n",
    "\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_blocks=3\n",
    "dataset = PolymerDataset(data_paths=[\"AA00400AA.npy\",\"AA66466AA.npy\"],num_blocks=num_blocks,lstm=True)\n",
    "num_features = dataset.data[0].shape[1]\n",
    "\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "train_size = int(0.8 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "train_data, test_data = random_split(dataset, [train_size, test_size])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=0/19, loss=0.7252147793769836, accuracy=50.019859313964844\n",
      "epoch=1/19, loss=0.7017585635185242, accuracy=49.46820068359375\n",
      "epoch=2/19, loss=0.693034827709198, accuracy=49.99779510498047\n",
      "epoch=3/19, loss=0.6864487528800964, accuracy=50.98636245727539\n",
      "epoch=4/19, loss=0.6845557689666748, accuracy=50.86720657348633\n",
      "epoch=5/19, loss=0.6677550673484802, accuracy=51.89549255371094\n",
      "epoch=6/19, loss=0.6874570846557617, accuracy=52.65457534790039\n",
      "epoch=7/19, loss=0.7309290766716003, accuracy=55.730613708496094\n",
      "epoch=8/19, loss=0.6609702706336975, accuracy=65.7884292602539\n",
      "epoch=9/19, loss=0.40221869945526123, accuracy=83.6356430053711\n",
      "epoch=10/19, loss=0.2085624486207962, accuracy=89.24488830566406\n",
      "epoch=11/19, loss=0.029638992622494698, accuracy=89.50086212158203\n",
      "epoch=12/19, loss=0.6779641509056091, accuracy=89.5802993774414\n",
      "epoch=13/19, loss=0.08710702508687973, accuracy=89.81861877441406\n",
      "epoch=14/19, loss=0.6329165101051331, accuracy=90.33055114746094\n",
      "epoch=15/19, loss=0.18310709297657013, accuracy=90.01280212402344\n",
      "epoch=16/19, loss=0.06562625616788864, accuracy=90.41881561279297\n",
      "epoch=17/19, loss=0.06775783002376556, accuracy=90.63507080078125\n",
      "epoch=18/19, loss=0.6102787852287292, accuracy=90.83808135986328\n",
      "epoch=19/19, loss=0.04958072304725647, accuracy=90.79835510253906\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = LSTM.train(dataset=train_data, num_features=num_features, num_blocks=num_blocks, hidden_dim=3, num_epochs=20, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 89.78%\n",
      "F1 Score: 89.47%\n",
      "Precision: 91.82%\n",
      "Recall: 87.23%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score,confusion_matrix\n",
    "\n",
    "data_loader = DataLoader(test_data, batch_size=64, shuffle=False)\n",
    "\n",
    "predictions = np.array([])\n",
    "labels = np.array([])\n",
    "\n",
    "with torch.no_grad():\n",
    "    for X, y in iter(data_loader):\n",
    "        probs = model(X)\n",
    "        preds = torch.argmax(probs, dim=1, keepdim=False)\n",
    "        predictions = np.concatenate((predictions,preds), axis=None)\n",
    "        labels= np.concatenate((labels,y),axis=None)\n",
    "\n",
    "\n",
    "accuracy = accuracy_score(labels,predictions)\n",
    "f1 = f1_score(labels,predictions)\n",
    "precision = precision_score(labels,predictions)\n",
    "recall = recall_score(labels,predictions)\n",
    "\n",
    "names =[\"Accuracy\", \"F1 Score\", \"Precision\", \"Recall\"]\n",
    "functions = [accuracy_score, f1_score, precision_score, recall_score]\n",
    "\n",
    "for name, func in zip(names,functions):\n",
    "    score = func(labels,predictions)\n",
    "    print(f\"{name}: {score*100:.2f}%\")\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e78a7ef29a5e3028f948eff69c34ba1d8ebd35a887497a02775c6aab840f6bc2"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
