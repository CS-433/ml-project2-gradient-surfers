{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "\n",
    "## classic pydata stack\n",
    "import os \n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "%matplotlib inline \n",
    "\n",
    "plt.style.use('ggplot')\n",
    "plt.rcParams['figure.figsize'] = (15,7)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "import vanilla_NN as NN\n",
    "\n",
    "sys.path.append('..')\n",
    "from pipeline import *\n",
    "from models import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing AA00400AA.npy: 100%|██████████| 14162/14162 [00:38<00:00, 370.47it/s]\n",
      "Processing AA66466AA.npy: 100%|██████████| 14185/14185 [02:40<00:00, 88.26it/s] \n"
     ]
    }
   ],
   "source": [
    "pipe = AA0066_Pipeline(num_blocks=1)\n",
    "\n",
    "dataset = PolymerDataset(data_paths=[\"AA00400AA.npy\",\"AA66466AA.npy\"],pipeline=pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(arr=dataset.labels.numpy(),file=\"backbone_bulky4_labels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kfold_cv_iter(y: np.ndarray, tx: np.ndarray, k: int = 5, seed: float = 1):\n",
    "    \"\"\"K-fold cross validation. Split data into k parts and iterate through the folds\n",
    "    Args:\n",
    "        y (np.ndarray): Label data\n",
    "        tx (np.ndarray): Features data\n",
    "        k (int, optional): Number of folds. Defaults to 5.\n",
    "        seed (float, optional): Seed for randomization. Defaults to 1.\n",
    "    Yields:\n",
    "        Generator: (x_train, y_train, x_test, y_test)\n",
    "    \"\"\"\n",
    "    num_row = y.shape[0]\n",
    "    fold_size = int(num_row / k)\n",
    "    np.random.seed(seed)\n",
    "    indices = np.random.permutation(num_row)\n",
    "    \n",
    "    for i in range(k):\n",
    "        test_indices = indices[i * fold_size: (i + 1) * fold_size]\n",
    "        train_indices = list(set(range(num_row)) - set(test_indices))\n",
    "        yield tx[train_indices], y[train_indices], tx[test_indices], y[test_indices]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "for i in range(1):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = np.zeros((9), dtype=bool)\n",
    "\n",
    "mask[0] = True\n",
    "\n",
    "test = tx[:,np.logical_not(mask)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3.51094612,  3.76399497, -0.74496833,  3.24895359,  0.67451326,\n",
       "       -0.94954394, -0.92173372, -0.89233789,  1.90161115])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tx[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.load(\"backbone_bulky4_labels.npy\",allow_pickle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random forest with removing a feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean :  0.9498006843757717\n",
      "median :  0.9479309979892051\n",
      "std :  0.9354076269093731\n",
      "min :  0.935301795604473\n",
      "max :  0.9464846368222387\n",
      "len :  0.9461318658059054\n",
      "count_extremums :  0.9495890217659717\n",
      "max_slope :  0.9508237203231383\n",
      "min_slope :  0.9491656965463718\n"
     ]
    }
   ],
   "source": [
    "names=[\"mean\", \"median\", \"std\", \"min\", \"max\", \"len\", \"count_extremums\", \"max_slope\", \"min_slope\"]\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "for i in range(9):\n",
    "    tx = np.load(\"backbone_bulky4_data_1.npy\",allow_pickle=True)\n",
    "    tx = tx.reshape((tx.shape[0],-1))\n",
    "    mask = np.zeros((9), dtype=bool)\n",
    "    mask[i] = True\n",
    "\n",
    "    tx =tx[:,np.logical_not(mask)]\n",
    "\n",
    "\n",
    "    iter = kfold_cv_iter(y=y,tx=tx,k=3,seed=2)\n",
    "    accuracies = []\n",
    "    for X_train, Y_train, X_test, Y_test in iter:\n",
    "        classifier = RandomForestClassifier(random_state=89,n_estimators = 100 )\n",
    "        classifier.fit(X_train, Y_train)\n",
    "        y_pred = classifier.predict(X_test)\n",
    "        accuracy = accuracy_score(Y_test, y_pred)\n",
    "        accuracies.append(accuracy)\n",
    "\n",
    "    print(names[i],\": \",np.array(accuracies).mean())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random forest with different num_blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_blocks=1 :  0.9506120577133382\n",
      "num_blocks=2 :  0.9566444420926378\n",
      "num_blocks=3 :  0.9583377429710375\n",
      "num_blocks=4 :  0.9586199597841042\n",
      "num_blocks=5 :  0.9594313331216707\n",
      "num_blocks=6 :  0.9579496948530708\n",
      "num_blocks=7 :  0.9571735986171376\n",
      "num_blocks=8 :  0.9562916710763044\n",
      "num_blocks=9 :  0.9564680565844711\n",
      "num_blocks=10 :  0.9506120577133382\n"
     ]
    }
   ],
   "source": [
    "for i in range(1,11):\n",
    "    path = f\"backbone_bulky4_data_{i}.npy\"\n",
    "    tx = np.load(path,allow_pickle=True)\n",
    "    tx = tx.reshape((tx.shape[0],-1))\n",
    "\n",
    "    iter = kfold_cv_iter(y=y,tx=tx,k=3,seed=2)\n",
    "    accuracies = []\n",
    "    for X_train, Y_train, X_test, Y_test in iter:\n",
    "        classifier = RandomForestClassifier(random_state=89,n_estimators = 100 )\n",
    "        classifier.fit(X_train, Y_train)\n",
    "        y_pred = classifier.predict(X_test)\n",
    "        accuracy = accuracy_score(Y_test, y_pred)\n",
    "        accuracies.append(accuracy)\n",
    "\n",
    "    print(f\"num_blocks={i}\",\": \",np.array(accuracies).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient boosting classifier with removing a feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean :  0.9448266130454721\n",
      "median :  0.9422866617278723\n",
      "std :  0.9335379405228067\n",
      "min :  0.9263766888912407\n",
      "max :  0.9379475782269728\n",
      "len :  0.9400289272233393\n",
      "count_extremums :  0.9446855046389389\n",
      "max_slope :  0.9445443962324056\n",
      "min_slope :  0.9424630472360391\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "names=[\"mean\", \"median\", \"std\", \"min\", \"max\", \"len\", \"count_extremums\", \"max_slope\", \"min_slope\"]\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "for i in range(9):\n",
    "    tx = np.load(\"backbone_bulky4_data_1.npy\",allow_pickle=True)\n",
    "    tx = tx.reshape((tx.shape[0],-1))\n",
    "    mask = np.zeros((9), dtype=bool)\n",
    "    mask[i] = True\n",
    "\n",
    "    tx =tx[:,np.logical_not(mask)]\n",
    "\n",
    "\n",
    "    iter = kfold_cv_iter(y=y,tx=tx,k=3,seed=2)\n",
    "    accuracies = []\n",
    "    for X_train, Y_train, X_test, Y_test in iter:\n",
    "        classifier =GradientBoostingClassifier(random_state=89,n_estimators = 100 )\n",
    "        classifier.fit(X_train, Y_train)\n",
    "        y_pred = classifier.predict(X_test)\n",
    "        accuracy = accuracy_score(Y_test, y_pred)\n",
    "        accuracies.append(accuracy)\n",
    "\n",
    "    print(names[i],\": \",np.array(accuracies).mean())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Gradient boosting classifier with different num_blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_blocks=1 :  0.944050516809539\n",
      "num_blocks=2 :  0.9525522983031713\n",
      "num_blocks=3 :  0.9549864183158712\n",
      "num_blocks=4 :  0.9551275267224044\n",
      "num_blocks=5 :  0.9546336472995378\n",
      "num_blocks=6 :  0.9557625145518044\n",
      "num_blocks=7 :  0.9553391893322045\n",
      "num_blocks=8 :  0.9553391893322045\n",
      "num_blocks=9 :  0.9547747557060712\n",
      "num_blocks=10 :  0.944050516809539\n"
     ]
    }
   ],
   "source": [
    "for i in range(1,11):\n",
    "    path = f\"backbone_bulky4_data_{i}.npy\"\n",
    "    tx = np.load(path,allow_pickle=True)\n",
    "    tx = tx.reshape((tx.shape[0],-1))\n",
    "\n",
    "    iter = kfold_cv_iter(y=y,tx=tx,k=3,seed=2)\n",
    "    accuracies = []\n",
    "    for X_train, Y_train, X_test, Y_test in iter:\n",
    "        classifier = GradientBoostingClassifier(random_state=89,n_estimators = 100 )\n",
    "        classifier.fit(X_train, Y_train)\n",
    "        y_pred = classifier.predict(X_test)\n",
    "        accuracy = accuracy_score(Y_test, y_pred)\n",
    "        accuracies.append(accuracy)\n",
    "\n",
    "    print(f\"num_blocks={i}\",\": \",np.array(accuracies).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PaddedDataset():\n",
    "\n",
    "    def __init__(self, data_paths,seed=10):\n",
    "        self.process(data_paths,seed)\n",
    "\n",
    "\n",
    "    def process(self,data_paths,seed):\n",
    "\n",
    "        raw_data = [np.load(data_path, allow_pickle=True) for data_path in data_paths]\n",
    "        labels = [0,1]\n",
    "\n",
    "    ## balance the dataset by removing signals that are too short or too long\n",
    "    ## first we build the dataframe to know the lengths of the time series\n",
    "\n",
    "        len_series = []\n",
    "\n",
    "        for data in raw_data:\n",
    "            lengths= []\n",
    "            for row in data:\n",
    "                ## length of time series\n",
    "                lengths.append(row.shape[0])\n",
    "\n",
    "            len_series.append(pd.Series(lengths))\n",
    "\n",
    "        ## enforces that the first dataset is the smaller one in total size\n",
    "        ## such that we can apply our balancing operations generally\n",
    "        if len(len_series[0]) > len(len_series[1]):\n",
    "            len_series.reverse()\n",
    "            raw_data.reverse()\n",
    "            labels.reverse()\n",
    "\n",
    "        ## filter the dataset and remove signals that are:\n",
    "        ## too short i.e. < len_series[0].quantile(0.1)\n",
    "        ## too long i.e. > len_series[0].quantile(0.9)\n",
    "        for i in range(2):\n",
    "            mask = (len_series[i] > max(len_series[0].quantile(0.1),num_blocks)) & (len_series[i] < len_series[0].quantile(0.5))\n",
    "            raw_data[i] = raw_data[i][mask]\n",
    "\n",
    "        ## most likely, one dataset is still bigger than the other one\n",
    "        ## therefore, we randomly sample data from the bigger dataset to create a new dataset of the same size as the small one \n",
    "        np.random.seed(seed=seed)\n",
    "\n",
    "        # making sure the smallest dataset is the first one\n",
    "        if len(raw_data[0]) > len(raw_data[1]):\n",
    "            raw_data.reverse()\n",
    "            labels.reverse()\n",
    "\n",
    "        # randomly sampling and making a balanced dataset\n",
    "        raw_data[1]  = np.random.permutation(raw_data[1])[:len(raw_data[0])]\n",
    "        data=[]\n",
    "        data_labels=[]\n",
    "        \n",
    "        max_length = max(len_series[0].max(), len_series[1].max())\n",
    "        print(\"Max length\",max_length)\n",
    "\n",
    "        ## using our Input class to build the entire dataset and extracting features from each row\n",
    "        for index, raw_data in enumerate(raw_data):\n",
    "            for raw_series in raw_data:\n",
    "                padded_series = self.pad(raw_series,max_length)\n",
    "                data.append(padded_series)\n",
    "                data_labels.append(labels[index])\n",
    "        data = np.array(data)\n",
    "\n",
    "        #normalizing features\n",
    "        data = (data - data.mean(axis=0)) / data.std(axis=0)\n",
    "        data_labels = np.array(data_labels)\n",
    "\n",
    "        self.data = data\n",
    "        self.labels = data_labels\n",
    "\n",
    "        return self\n",
    "\n",
    "    \n",
    "    def pad(self,raw_series, max_length):\n",
    "\n",
    "        l = max_length - raw_series.shape[0]\n",
    "        x=raw_series[:,0]\n",
    "        y= raw_series[:,1]\n",
    "\n",
    "        if l > 0:\n",
    "            res1 = np.concatenate((x, np.linspace(start= x[-1] + 0.01, stop= x[-1] + l*0.01 , num=l)),axis=None)\n",
    "            res2 = np.concatenate((y, np.zeros(l)),axis=None)\n",
    "\n",
    "        res = np.vstack((res1,res2)).T\n",
    "\n",
    "        return res\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_11244/951549173.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPaddedDataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_paths\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"AA00400AA.npy\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"AA66466AA.npy\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_11244/59852541.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, data_paths, seed)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_paths\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mseed\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprocess\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_paths\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mseed\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_11244/59852541.py\u001b[0m in \u001b[0;36mprocess\u001b[1;34m(self, data_paths, seed)\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mprocess\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdata_paths\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mseed\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m         \u001b[0mraw_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mallow_pickle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mdata_path\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdata_paths\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m         \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_11244/59852541.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mprocess\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdata_paths\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mseed\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m         \u001b[0mraw_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mallow_pickle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mdata_path\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdata_paths\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m         \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\numpy\\lib\\npyio.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(file, mmap_mode, allow_pickle, fix_imports, encoding)\u001b[0m\n\u001b[0;32m    438\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mformat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen_memmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmmap_mode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    439\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 440\u001b[1;33m                 return format.read_array(fid, allow_pickle=allow_pickle,\n\u001b[0m\u001b[0;32m    441\u001b[0m                                          pickle_kwargs=pickle_kwargs)\n\u001b[0;32m    442\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\numpy\\lib\\format.py\u001b[0m in \u001b[0;36mread_array\u001b[1;34m(fp, allow_pickle, pickle_kwargs)\u001b[0m\n\u001b[0;32m    746\u001b[0m             \u001b[0mpickle_kwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    747\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 748\u001b[1;33m             \u001b[0marray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mpickle_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    749\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mUnicodeError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    750\u001b[0m             \u001b[1;31m# Friendlier error message\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "dataset = PaddedDataset(data_paths=[\"AA00400AA.npy\",\"AA66466AA.npy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "If passed as a np.array, X must be a 3-dimensional array, but found shape: (21243, 27)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_11244/4255915795.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mclassifier\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTimeSeriesForestClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0mclassifier\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sktime\\series_as_features\\base\\estimators\\interval_based\\_tsf.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m     85\u001b[0m             \u001b[0mAn\u001b[0m \u001b[0mfitted\u001b[0m \u001b[0minstance\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m         \"\"\"\n\u001b[1;32m---> 87\u001b[1;33m         X, y = check_X_y(\n\u001b[0m\u001b[0;32m     88\u001b[0m             \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m             \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sktime\\utils\\validation\\panel.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[1;34m(X, y, enforce_univariate, enforce_min_instances, enforce_min_columns, coerce_to_numpy, coerce_to_pandas)\u001b[0m\n\u001b[0;32m    186\u001b[0m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    187\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 188\u001b[1;33m     X = check_X(\n\u001b[0m\u001b[0;32m    189\u001b[0m         \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    190\u001b[0m         \u001b[0menforce_univariate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0menforce_univariate\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sktime\\utils\\validation\\panel.py\u001b[0m in \u001b[0;36mcheck_X\u001b[1;34m(X, enforce_univariate, enforce_min_instances, enforce_min_columns, coerce_to_numpy, coerce_to_pandas)\u001b[0m\n\u001b[0;32m     72\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 74\u001b[1;33m             raise ValueError(\n\u001b[0m\u001b[0;32m     75\u001b[0m                 \u001b[1;34mf\"If passed as a np.array, X must be a 3-dimensional \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m                 \u001b[1;34mf\"array, but found shape: {X.shape}\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: If passed as a np.array, X must be a 3-dimensional array, but found shape: (21243, 27)"
     ]
    }
   ],
   "source": [
    "from sktime.classification.interval_based import TimeSeriesForestClassifier\n",
    "from sktime.datasets import load_arrow_head\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "X, y = dataset.data, dataset.labels\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "classifier = TimeSeriesForestClassifier()\n",
    "classifier.fit(X_train, y_train)\n",
    "y_pred = classifier.predict(X_test)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.17944997, -0.20482437,  0.67186573, -0.00658525,  0.94489608,\n",
       "       -0.81025791, -0.73619943,  1.31484999, -0.43366814, -0.71692991,\n",
       "       -0.57235341, -0.45231736, -0.82291628, -0.43769871, -0.81025791,\n",
       "       -0.89015383,  1.00854561, -1.42100356, -0.48263948, -1.11383077,\n",
       "        0.41667791, -0.81664598,  0.96985427, -0.81025791, -0.71873826,\n",
       "        1.37042307, -0.68925605])"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.data[1]"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e78a7ef29a5e3028f948eff69c34ba1d8ebd35a887497a02775c6aab840f6bc2"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
