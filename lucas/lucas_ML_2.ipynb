{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "\n",
    "## classic pydata stack\n",
    "import os \n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "%matplotlib inline \n",
    "\n",
    "plt.style.use('ggplot')\n",
    "plt.rcParams['figure.figsize'] = (15,7)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "import vanilla_NN as NN\n",
    "\n",
    "sys.path.append('..')\n",
    "from pipeline import *\n",
    "from models import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def a():\n",
    "    return 1,2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4, 1, 2, 5]\n"
     ]
    }
   ],
   "source": [
    "l=[4,*a(),5]\n",
    "print(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing ../data/AA66266AA.npy: 100%|██████████| 17594/17594 [00:59<00:00, 294.32it/s]\n",
      "Processing ../data/AA662266AA.npy: 100%|██████████| 17625/17625 [00:49<00:00, 359.14it/s]\n"
     ]
    }
   ],
   "source": [
    "pipe = PairSingle_Pipeline(num_blocks=6)\n",
    "dataset = PolymerDataset(data_paths=[\"../data/AA66266AA.npy\",\"../data/AA662266AA.npy\"],pipeline=pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = train_test_split(dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VanillaLSTM(input_dim=dataset.num_features, output_dim=dataset.num_classes, num_layers=1, hidden_dim=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-20 16:42:15.626977: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2021-12-20 16:42:15.626998: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/lucastrg/ml4science-polymers/runs/2q106tah\" target=\"_blank\">crisp-shape-38</a></strong> to <a href=\"https://wandb.ai/lucastrg/ml4science-polymers\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=0/30, loss=0.10845699613647801, accuracy=95.88301849365234\n",
      "epoch=1/30, loss=0.10912274822060551, accuracy=95.85462951660156\n",
      "epoch=2/30, loss=0.10495774421308722, accuracy=95.87947082519531\n",
      "epoch=3/30, loss=0.10428129143214651, accuracy=95.91141510009766\n",
      "epoch=4/30, loss=0.10404094263711679, accuracy=95.94690704345703\n",
      "epoch=5/30, loss=0.10395806796649205, accuracy=95.91851043701172\n",
      "epoch=6/30, loss=0.10833293399108308, accuracy=95.92205810546875\n",
      "epoch=7/30, loss=0.10485011598627482, accuracy=95.91141510009766\n",
      "epoch=8/30, loss=0.10548055065529686, accuracy=95.9540023803711\n",
      "epoch=9/30, loss=0.10704067801790577, accuracy=95.92205810546875\n",
      "epoch=10/30, loss=0.1047423624300531, accuracy=95.98594665527344\n",
      "epoch=11/30, loss=0.10847828696881022, accuracy=95.97174835205078\n",
      "epoch=12/30, loss=0.10542138898745179, accuracy=95.9362564086914\n",
      "epoch=13/30, loss=0.10399722258028175, accuracy=95.97529602050781\n",
      "epoch=14/30, loss=0.10402360096174691, accuracy=96.0001449584961\n",
      "epoch=15/30, loss=0.10601290741137095, accuracy=95.97174835205078\n",
      "epoch=16/30, loss=0.1045587771971311, accuracy=95.98949432373047\n",
      "epoch=17/30, loss=0.10441536802266325, accuracy=95.97884368896484\n",
      "epoch=18/30, loss=0.10339212246305708, accuracy=95.97174835205078\n",
      "epoch=19/30, loss=0.10844305876110281, accuracy=95.97174835205078\n",
      "epoch=20/30, loss=0.10422032033758503, accuracy=95.90076446533203\n",
      "epoch=21/30, loss=0.10466684600604433, accuracy=95.9930419921875\n",
      "epoch=22/30, loss=0.10430958567719374, accuracy=95.90076446533203\n",
      "epoch=23/30, loss=0.10407835418092352, accuracy=95.91851043701172\n",
      "epoch=24/30, loss=0.1038016992887216, accuracy=95.95755004882812\n",
      "epoch=25/30, loss=0.10512128937989473, accuracy=95.97529602050781\n",
      "epoch=26/30, loss=0.10498950071632862, accuracy=95.92916107177734\n",
      "epoch=27/30, loss=0.10415051032656006, accuracy=96.02498626708984\n",
      "epoch=28/30, loss=0.10505545059485095, accuracy=95.91141510009766\n",
      "epoch=29/30, loss=0.10392177158168384, accuracy=95.90786743164062\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish, PID 1398048... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">crisp-shape-38</strong>: <a href=\"https://wandb.ai/lucastrg/ml4science-polymers/runs/2q106tah\" target=\"_blank\">https://wandb.ai/lucastrg/ml4science-polymers/runs/2q106tah</a><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model, test_metrics = train(train_data, model, batch_size=512, num_epochs=200, lr_rate=0.005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.9532869515831321,\n",
       " 'f1_score': 0.9536815430099959,\n",
       " 'precision': 0.9589467723669309,\n",
       " 'recall': 0.948473816858023,\n",
       " 'confusion_matrix': array([[0.958, 0.042],\n",
       "        [0.052, 0.948]])}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test(test_data,model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(arr=dataset.labels.numpy(),file=\"backbone_bulky4_labels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kfold_cv_iter(y: np.ndarray, tx: np.ndarray, k: int = 5, seed: float = 1):\n",
    "    \"\"\"K-fold cross validation. Split data into k parts and iterate through the folds\n",
    "    Args:\n",
    "        y (np.ndarray): Label data\n",
    "        tx (np.ndarray): Features data\n",
    "        k (int, optional): Number of folds. Defaults to 5.\n",
    "        seed (float, optional): Seed for randomization. Defaults to 1.\n",
    "    Yields:\n",
    "        Generator: (x_train, y_train, x_test, y_test)\n",
    "    \"\"\"\n",
    "    num_row = y.shape[0]\n",
    "    fold_size = int(num_row / k)\n",
    "    np.random.seed(seed)\n",
    "    indices = np.random.permutation(num_row)\n",
    "    \n",
    "    for i in range(k):\n",
    "        test_indices = indices[i * fold_size: (i + 1) * fold_size]\n",
    "        train_indices = list(set(range(num_row)) - set(test_indices))\n",
    "        yield tx[train_indices], y[train_indices], tx[test_indices], y[test_indices]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3716926/4037995355.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfull_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/FLEP/MA1/ML/ml4science-polymers/ml4science/lib/python3.9/site-packages/torch/utils/data/dataset.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, attribute_name)\u001b[0m\n\u001b[1;32m     81\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(dataset.full_dataset.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "for i in dataset.full_dataset\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tx' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2546599/1865804977.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogical_not\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'tx' is not defined"
     ]
    }
   ],
   "source": [
    "mask = np.zeros((9), dtype=bool)\n",
    "\n",
    "mask[0] = True\n",
    "\n",
    "test = tx[:,np.logical_not(mask)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3.51094612,  3.76399497, -0.74496833,  3.24895359,  0.67451326,\n",
       "       -0.94954394, -0.92173372, -0.89233789,  1.90161115])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tx[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.load(\"backbone_bulky4_labels.npy\",allow_pickle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random forest with removing a feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'backbone_bulky4_data_1.npy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2546599/2642525114.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m9\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mtx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"backbone_bulky4_data_1.npy\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mallow_pickle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mtx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m9\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/FLEP/MA1/ML/ml4science-polymers/ml4science/lib/python3.9/site-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding)\u001b[0m\n\u001b[1;32m    415\u001b[0m             \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    416\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 417\u001b[0;31m             \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menter_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos_fspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    418\u001b[0m             \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'backbone_bulky4_data_1.npy'"
     ]
    }
   ],
   "source": [
    "names=[\"mean\", \"median\", \"std\", \"min\", \"max\", \"len\", \"count_extremums\", \"max_slope\", \"min_slope\"]\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "for i in range(9):\n",
    "    tx = np.load(\"backbone_bulky4_data_1.npy\",allow_pickle=True)\n",
    "    tx = tx.reshape((tx.shape[0],-1))\n",
    "    mask = np.zeros((9), dtype=bool)\n",
    "    mask[i] = True\n",
    "\n",
    "    tx =tx[:,np.logical_not(mask)]\n",
    "\n",
    "\n",
    "    iter = kfold_cv_iter(y=y,tx=tx,k=3,seed=2)\n",
    "    accuracies = []\n",
    "    for X_train, Y_train, X_test, Y_test in iter:\n",
    "        classifier = RandomForestClassifier(random_state=89,n_estimators = 100 )\n",
    "        classifier.fit(X_train, Y_train)\n",
    "        y_pred = classifier.predict(X_test)\n",
    "        accuracy = accuracy_score(Y_test, y_pred)\n",
    "        accuracies.append(accuracy)\n",
    "\n",
    "    print(names[i],\": \",np.array(accuracies).mean())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random forest with different num_blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'backbone_bulky4_data_1.npy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2546599/905070461.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m11\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"backbone_bulky4_data_{i}.npy\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mtx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mallow_pickle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mtx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/FLEP/MA1/ML/ml4science-polymers/ml4science/lib/python3.9/site-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding)\u001b[0m\n\u001b[1;32m    415\u001b[0m             \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    416\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 417\u001b[0;31m             \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menter_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos_fspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    418\u001b[0m             \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'backbone_bulky4_data_1.npy'"
     ]
    }
   ],
   "source": [
    "for i in range(1,11):\n",
    "    path = f\"backbone_bulky4_data_{i}.npy\"\n",
    "    tx = np.load(path,allow_pickle=True)\n",
    "    tx = tx.reshape((tx.shape[0],-1))\n",
    "\n",
    "    iter = kfold_cv_iter(y=y,tx=tx,k=3,seed=2)\n",
    "    accuracies = []\n",
    "    for X_train, Y_train, X_test, Y_test in iter:\n",
    "        classifier = RandomForestClassifier(random_state=89,n_estimators = 100 )\n",
    "        classifier.fit(X_train, Y_train)\n",
    "        y_pred = classifier.predict(X_test)\n",
    "        accuracy = accuracy_score(Y_test, y_pred)\n",
    "        accuracies.append(accuracy)\n",
    "\n",
    "    print(f\"num_blocks={i}\",\": \",np.array(accuracies).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient boosting classifier with removing a feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean :  0.9448266130454721\n",
      "median :  0.9422866617278723\n",
      "std :  0.9335379405228067\n",
      "min :  0.9263766888912407\n",
      "max :  0.9379475782269728\n",
      "len :  0.9400289272233393\n",
      "count_extremums :  0.9446855046389389\n",
      "max_slope :  0.9445443962324056\n",
      "min_slope :  0.9424630472360391\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "names=[\"mean\", \"median\", \"std\", \"min\", \"max\", \"len\", \"count_extremums\", \"max_slope\", \"min_slope\"]\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "for i in range(9):\n",
    "    tx = np.load(\"backbone_bulky4_data_1.npy\",allow_pickle=True)\n",
    "    tx = tx.reshape((tx.shape[0],-1))\n",
    "    mask = np.zeros((9), dtype=bool)\n",
    "    mask[i] = True\n",
    "\n",
    "    tx =tx[:,np.logical_not(mask)]\n",
    "\n",
    "\n",
    "    iter = kfold_cv_iter(y=y,tx=tx,k=3,seed=2)\n",
    "    accuracies = []\n",
    "    for X_train, Y_train, X_test, Y_test in iter:\n",
    "        classifier =GradientBoostingClassifier(random_state=89,n_estimators = 100 )\n",
    "        classifier.fit(X_train, Y_train)\n",
    "        y_pred = classifier.predict(X_test)\n",
    "        accuracy = accuracy_score(Y_test, y_pred)\n",
    "        accuracies.append(accuracy)\n",
    "\n",
    "    print(names[i],\": \",np.array(accuracies).mean())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Gradient boosting classifier with different num_blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_blocks=1 :  0.944050516809539\n",
      "num_blocks=2 :  0.9525522983031713\n",
      "num_blocks=3 :  0.9549864183158712\n",
      "num_blocks=4 :  0.9551275267224044\n",
      "num_blocks=5 :  0.9546336472995378\n",
      "num_blocks=6 :  0.9557625145518044\n",
      "num_blocks=7 :  0.9553391893322045\n",
      "num_blocks=8 :  0.9553391893322045\n",
      "num_blocks=9 :  0.9547747557060712\n",
      "num_blocks=10 :  0.944050516809539\n"
     ]
    }
   ],
   "source": [
    "for i in range(1,11):\n",
    "    path = f\"backbone_bulky4_data_{i}.npy\"\n",
    "    tx = np.load(path,allow_pickle=True)\n",
    "    tx = tx.reshape((tx.shape[0],-1))\n",
    "\n",
    "    iter = kfold_cv_iter(y=y,tx=tx,k=3,seed=2)\n",
    "    accuracies = []\n",
    "    for X_train, Y_train, X_test, Y_test in iter:\n",
    "        classifier = GradientBoostingClassifier(random_state=89,n_estimators = 100 )\n",
    "        classifier.fit(X_train, Y_train)\n",
    "        y_pred = classifier.predict(X_test)\n",
    "        accuracy = accuracy_score(Y_test, y_pred)\n",
    "        accuracies.append(accuracy)\n",
    "\n",
    "    print(f\"num_blocks={i}\",\": \",np.array(accuracies).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PaddedDataset():\n",
    "\n",
    "    def __init__(self, data_paths,seed=10):\n",
    "        self.process(data_paths,seed)\n",
    "\n",
    "\n",
    "    def process(self,data_paths,seed):\n",
    "\n",
    "        raw_data = [np.load(data_path, allow_pickle=True) for data_path in data_paths]\n",
    "        labels = [0,1]\n",
    "\n",
    "    ## balance the dataset by removing signals that are too short or too long\n",
    "    ## first we build the dataframe to know the lengths of the time series\n",
    "\n",
    "        len_series = []\n",
    "\n",
    "        for data in raw_data:\n",
    "            lengths= []\n",
    "            for row in data:\n",
    "                ## length of time series\n",
    "                lengths.append(row.shape[0])\n",
    "\n",
    "            len_series.append(pd.Series(lengths))\n",
    "\n",
    "        ## enforces that the first dataset is the smaller one in total size\n",
    "        ## such that we can apply our balancing operations generally\n",
    "        if len(len_series[0]) > len(len_series[1]):\n",
    "            len_series.reverse()\n",
    "            raw_data.reverse()\n",
    "            labels.reverse()\n",
    "\n",
    "        ## filter the dataset and remove signals that are:\n",
    "        ## too short i.e. < len_series[0].quantile(0.1)\n",
    "        ## too long i.e. > len_series[0].quantile(0.9)\n",
    "        for i in range(2):\n",
    "            mask = (len_series[i] > max(len_series[0].quantile(0.1),num_blocks)) & (len_series[i] < len_series[0].quantile(0.5))\n",
    "            raw_data[i] = raw_data[i][mask]\n",
    "\n",
    "        ## most likely, one dataset is still bigger than the other one\n",
    "        ## therefore, we randomly sample data from the bigger dataset to create a new dataset of the same size as the small one \n",
    "        np.random.seed(seed=seed)\n",
    "\n",
    "        # making sure the smallest dataset is the first one\n",
    "        if len(raw_data[0]) > len(raw_data[1]):\n",
    "            raw_data.reverse()\n",
    "            labels.reverse()\n",
    "\n",
    "        # randomly sampling and making a balanced dataset\n",
    "        raw_data[1]  = np.random.permutation(raw_data[1])[:len(raw_data[0])]\n",
    "        data=[]\n",
    "        data_labels=[]\n",
    "        \n",
    "        max_length = max(len_series[0].max(), len_series[1].max())\n",
    "        print(\"Max length\",max_length)\n",
    "\n",
    "        ## using our Input class to build the entire dataset and extracting features from each row\n",
    "        for index, raw_data in enumerate(raw_data):\n",
    "            for raw_series in raw_data:\n",
    "                padded_series = self.pad(raw_series,max_length)\n",
    "                data.append(padded_series)\n",
    "                data_labels.append(labels[index])\n",
    "        data = np.array(data)\n",
    "\n",
    "        #normalizing features\n",
    "        data = (data - data.mean(axis=0)) / data.std(axis=0)\n",
    "        data_labels = np.array(data_labels)\n",
    "\n",
    "        self.data = data\n",
    "        self.labels = data_labels\n",
    "\n",
    "        return self\n",
    "\n",
    "    \n",
    "    def pad(self,raw_series, max_length):\n",
    "\n",
    "        l = max_length - raw_series.shape[0]\n",
    "        x=raw_series[:,0]\n",
    "        y= raw_series[:,1]\n",
    "\n",
    "        if l > 0:\n",
    "            res1 = np.concatenate((x, np.linspace(start= x[-1] + 0.01, stop= x[-1] + l*0.01 , num=l)),axis=None)\n",
    "            res2 = np.concatenate((y, np.zeros(l)),axis=None)\n",
    "\n",
    "        res = np.vstack((res1,res2)).T\n",
    "\n",
    "        return res\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_11244/951549173.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPaddedDataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_paths\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"AA00400AA.npy\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"AA66466AA.npy\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_11244/59852541.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, data_paths, seed)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_paths\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mseed\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprocess\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_paths\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mseed\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_11244/59852541.py\u001b[0m in \u001b[0;36mprocess\u001b[1;34m(self, data_paths, seed)\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mprocess\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdata_paths\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mseed\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m         \u001b[0mraw_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mallow_pickle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mdata_path\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdata_paths\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m         \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_11244/59852541.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mprocess\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdata_paths\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mseed\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m         \u001b[0mraw_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mallow_pickle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mdata_path\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdata_paths\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m         \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\numpy\\lib\\npyio.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(file, mmap_mode, allow_pickle, fix_imports, encoding)\u001b[0m\n\u001b[0;32m    438\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mformat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen_memmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmmap_mode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    439\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 440\u001b[1;33m                 return format.read_array(fid, allow_pickle=allow_pickle,\n\u001b[0m\u001b[0;32m    441\u001b[0m                                          pickle_kwargs=pickle_kwargs)\n\u001b[0;32m    442\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\numpy\\lib\\format.py\u001b[0m in \u001b[0;36mread_array\u001b[1;34m(fp, allow_pickle, pickle_kwargs)\u001b[0m\n\u001b[0;32m    746\u001b[0m             \u001b[0mpickle_kwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    747\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 748\u001b[1;33m             \u001b[0marray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mpickle_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    749\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mUnicodeError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    750\u001b[0m             \u001b[1;31m# Friendlier error message\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "dataset = PaddedDataset(data_paths=[\"AA00400AA.npy\",\"AA66466AA.npy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "If passed as a np.array, X must be a 3-dimensional array, but found shape: (21243, 27)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_11244/4255915795.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mclassifier\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTimeSeriesForestClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0mclassifier\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sktime\\series_as_features\\base\\estimators\\interval_based\\_tsf.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m     85\u001b[0m             \u001b[0mAn\u001b[0m \u001b[0mfitted\u001b[0m \u001b[0minstance\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m         \"\"\"\n\u001b[1;32m---> 87\u001b[1;33m         X, y = check_X_y(\n\u001b[0m\u001b[0;32m     88\u001b[0m             \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m             \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sktime\\utils\\validation\\panel.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[1;34m(X, y, enforce_univariate, enforce_min_instances, enforce_min_columns, coerce_to_numpy, coerce_to_pandas)\u001b[0m\n\u001b[0;32m    186\u001b[0m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    187\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 188\u001b[1;33m     X = check_X(\n\u001b[0m\u001b[0;32m    189\u001b[0m         \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    190\u001b[0m         \u001b[0menforce_univariate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0menforce_univariate\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sktime\\utils\\validation\\panel.py\u001b[0m in \u001b[0;36mcheck_X\u001b[1;34m(X, enforce_univariate, enforce_min_instances, enforce_min_columns, coerce_to_numpy, coerce_to_pandas)\u001b[0m\n\u001b[0;32m     72\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 74\u001b[1;33m             raise ValueError(\n\u001b[0m\u001b[0;32m     75\u001b[0m                 \u001b[1;34mf\"If passed as a np.array, X must be a 3-dimensional \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m                 \u001b[1;34mf\"array, but found shape: {X.shape}\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: If passed as a np.array, X must be a 3-dimensional array, but found shape: (21243, 27)"
     ]
    }
   ],
   "source": [
    "from sktime.classification.interval_based import TimeSeriesForestClassifier\n",
    "from sktime.datasets import load_arrow_head\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "X, y = dataset.data, dataset.labels\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "classifier = TimeSeriesForestClassifier()\n",
    "classifier.fit(X_train, y_train)\n",
    "y_pred = classifier.predict(X_test)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.17944997, -0.20482437,  0.67186573, -0.00658525,  0.94489608,\n",
       "       -0.81025791, -0.73619943,  1.31484999, -0.43366814, -0.71692991,\n",
       "       -0.57235341, -0.45231736, -0.82291628, -0.43769871, -0.81025791,\n",
       "       -0.89015383,  1.00854561, -1.42100356, -0.48263948, -1.11383077,\n",
       "        0.41667791, -0.81664598,  0.96985427, -0.81025791, -0.71873826,\n",
       "        1.37042307, -0.68925605])"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.data[1]"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e78a7ef29a5e3028f948eff69c34ba1d8ebd35a887497a02775c6aab840f6bc2"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
